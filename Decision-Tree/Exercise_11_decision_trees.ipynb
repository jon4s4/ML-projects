{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data=None, children=None, split_on=None, pred_class=None):\n",
    "        self.children = children\n",
    "        self.split_on = split_on\n",
    "        self.pred_class = pred_class\n",
    "        self.is_leaf = False\n",
    "\n",
    "    def __str__(self):\n",
    "        s = f\"Node({id(self)}\"\n",
    "        if self.children:\n",
    "            s += f\", children: {[id(c) for c in self.children]}\"\n",
    "        if self.split_on:\n",
    "            s += f\", split on: {self.split_on}\"\n",
    "        if self.pred_class:\n",
    "            s += f\", predicted class {self.pred_class})\"\n",
    "        return s\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self):\n",
    "        self.root = Node()\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(Y):\n",
    "        \"\"\"\n",
    "        Calculate the entropy of a numpy array of target values\n",
    "        \"\"\"\n",
    "        # Implement\n",
    "        _, counts = np.unique(Y, return_counts=True)\n",
    "\n",
    "        probabilities = counts / len(Y)\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy\n",
    "        \n",
    "        \n",
    "\n",
    "    def split_on_feature(self, data, feat_index):\n",
    "        \"\"\"\n",
    "        Find best attribute for splitting the data.\n",
    "        Return the nodes into which data has been split, the data split and the weighted entropy of the split.\n",
    "        split nodes and split data are dictionaries where the keys is the value to split on.\n",
    "        \"\"\"\n",
    "\n",
    "        split_nodes = {}\n",
    "        split_data = {}\n",
    "        weighted_entropy = 0\n",
    "        \n",
    "        # Implement here\n",
    "        total_samples = len(data)\n",
    "\n",
    "        for value in np.unique(data[:, feat_index]):\n",
    "            subset = data[data[:, feat_index] == value]\n",
    "            target_values = subset[:, -1]\n",
    "            subset_entropy = DecisionTreeClassifier.entropy(target_values)\n",
    "            weight = len(subset) / total_samples\n",
    "            weighted_entropy += weight * subset_entropy\n",
    "            split_nodes[value] = target_values\n",
    "            split_data[value] = subset\n",
    "\n",
    "        return split_nodes, split_data, weighted_entropy\n",
    "\n",
    "    def best_split(self, node, data, depth_left):\n",
    "        # Check if the node meets the criteria to stop splitting\n",
    "        if depth_left == 0 or self.meet_criteria(data):\n",
    "            node.is_leaf = True\n",
    "            y = self.get_y(data)\n",
    "            node.pred_class = self.get_pred_class(y)\n",
    "            return\n",
    "\n",
    "        # Find best attribute to split data on and record it as well as the children nodes\n",
    "        # Implement here\n",
    "        best_feature = None\n",
    "        best_split_nodes = None\n",
    "        best_split_data = None\n",
    "        lowest_weighted_entropy = float('inf')\n",
    "\n",
    "        num_features = data.shape[1] - 1\n",
    "        for feat_index in range(num_features):\n",
    "            split_nodes, split_data, weighted_entropie  = self.split_on_feature(data, feat_index)\n",
    "\n",
    "            if weighted_entropie < lowest_weighted_entropy:\n",
    "                lowest_weighted_entropy = weighted_entropie\n",
    "                best_feature = feat_index\n",
    "                best_split_nodes = split_nodes\n",
    "                best_split_data = split_data\n",
    "                \n",
    "\n",
    "        node.children = {}\n",
    "        node.split_on = best_feature\n",
    "\n",
    "        # Recursively call the best_split function for each child node\n",
    "        # Implement here\n",
    "        for value, subset in best_split_data.items():\n",
    "            child_node = Node()\n",
    "            node.children[value] = child_node\n",
    "            self.best_split(child_node, subset, depth_left - 1)\n",
    "\n",
    "\n",
    "    def meet_criteria(self, data):\n",
    "        \"\"\"\n",
    "        Check if the criteria for stopping the tree expansion is met for a given node. Here we only check if the entropy of the target values (y) is zero.\n",
    "        Additionally, you can customize criteria based on your specific requirements. For instance, you can set the maximum depth for the decision tree or incorporate other conditions for stopping the tree expansion. Modify the implementation of this method according to your desired criteria.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : np.array\n",
    "            The data to check for meeting the stopping criteria.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        bool\n",
    "            True if the criteria is met, False otherwise.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        y = self.get_y(data)\n",
    "        return True if self.entropy(y) == 0 else False\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_y(data):\n",
    "        y = data[:, -1]\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pred_class(Y):\n",
    "        labels, labels_counts = np.unique(Y, return_counts=True)\n",
    "        index = np.argmax(labels_counts)\n",
    "        return labels[index]\n",
    "\n",
    "    def fit(self, X, Y, max_depth=int(1e10)):\n",
    "        \"\"\"\n",
    "        Fit the decision tree model to the provided dataset.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: numpy.ndarray\n",
    "            The input features of the dataset.\n",
    "\n",
    "        Y: numpy.ndarray\n",
    "            The target labels of the dataset.\n",
    "        \"\"\"\n",
    "        data = np.column_stack([X, Y])\n",
    "        self.root = Node()\n",
    "        self.best_split(self.root, data, max_depth)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([self.traverse_tree(x, self.root) for x in X])\n",
    "        return predictions\n",
    "\n",
    "    def traverse_tree(self, x, node):\n",
    "        # Check if the current node is a leaf node\n",
    "        if node.is_leaf:\n",
    "            return node.pred_class\n",
    "\n",
    "        # Get the feature value at the split point for the current node\n",
    "        feat_value = x[node.split_on]\n",
    "\n",
    "        # Recursively traverse the decision tree using the child node corresponding to the feature value\n",
    "        predicted_class = self.traverse_tree(x, node.children[feat_value])\n",
    "\n",
    "        return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1998)\n",
      "(100, 1998)\n",
      "(100, 1998)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=np.load(\"Features.npy\")\n",
    "y=np.load(\"Labels.npy\")\n",
    "\n",
    "\n",
    "# binarize the features: use all possible values as thresholds on all axes\n",
    "def axis_aligned_features(X):\n",
    "    X_features = None\n",
    "    for i in range(X.shape[1]):\n",
    "        distinct_vals = sorted(np.unique(X[:,i]))\n",
    "        thresholds = [(x + y)/2 for x,y in zip(iter(distinct_vals), iter(distinct_vals[1::]))]\n",
    "        for t in thresholds:\n",
    "            mask = X[:,i] > t\n",
    "            if X_features is not None:\n",
    "                X_features = np.concatenate([X_features, mask.reshape((-1,1))], axis=1)\n",
    "            else:\n",
    "                X_features = mask.reshape((-1,1))\n",
    "    return X_features\n",
    "\n",
    "X = axis_aligned_features(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=100, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=100, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 1.0\n",
      "Validation set accuracy: 0.91\n",
      "Test set accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "def accuracy(dt, X, y):\n",
    "    acc = (dt.predict(X) == y).sum() / len(y)\n",
    "    return acc\n",
    "\n",
    "# get validation set accuracy\n",
    "print(f\"Training set accuracy: {accuracy(dt, X_train, y_train)}\")\n",
    "print(f\"Validation set accuracy: {accuracy(dt, X_val, y_val)}\")\n",
    "print(f\"Test set accuracy: {accuracy(dt, X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn random forest\n",
    "from scipy.stats import mode\n",
    "\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self):\n",
    "        self.trees = []\n",
    "    \n",
    "    def bootstrap_sample(self, X, y, sample_frac):\n",
    "        indices = np.arange(X.shape[0])\n",
    "        indices = np.random.permutation(indices)\n",
    "        indices = indices[:round(sample_frac*len(indices))]\n",
    "        X = X[indices,:]\n",
    "        y = y[indices]\n",
    "        return X,y\n",
    "\n",
    "    def bootstrap_features(self, X, feature_frac):\n",
    "        indices = np.arange(X.shape[1])\n",
    "        indices = np.random.permutation(indices)\n",
    "        indices = indices[:round(feature_frac*len(indices))]\n",
    "        assert not (X == -1).any()\n",
    "        X[:,indices] = -1 # we should never split on -1, since this would not differentiate\n",
    "        return X\n",
    "    \n",
    "    def fit(self, X, y, n_trees, max_depth, sample_frac=0.5, feature_frac=0.5):\n",
    "        for i in range(n_trees):\n",
    "            # Train DT and append to list\n",
    "            # Implement here\n",
    "            self.trees = []\n",
    "            n_samples, n_features = X.shape\n",
    "\n",
    "            for i in range(n_trees):\n",
    "                sample_indices = np.random.choice(n_samples, int(sample_frac * n_samples), replace=True)\n",
    "                X_sample = X[sample_indices]\n",
    "                y_sample = y[sample_indices]\n",
    "\n",
    "                feature_indices = np.random.choice(n_features, int(feature_frac * n_features), replace=True)\n",
    "                X_sample = X_sample[:, feature_indices]\n",
    "\n",
    "                tree = DecisionTreeClassifier()\n",
    "                tree.fit(X_sample, y_sample, max_depth=max_depth)\n",
    "                self.trees.append((tree, feature_indices))\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Get majority vote from all trees\n",
    "        # Implement here\n",
    "        predictions = np.zeros((len(self.trees), X.shape[0]), dtype=int)\n",
    "\n",
    "        for i, (tree, feature_indices) in enumerate(self.trees):\n",
    "            X_subset = X[:, feature_indices]\n",
    "            predictions[i] = tree.predict(X_subset)\n",
    "\n",
    "        majority_vote = mode(predictions, axis=0).mode[0]\n",
    "        return majority_vote\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train, n_trees=10, max_depth=10, sample_frac=0.5, feature_frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.395\n",
      "Validation set accuracy: 0.38\n",
      "Test set accuracy: 0.4\n"
     ]
    }
   ],
   "source": [
    "# get validation set accuracy\n",
    "print(f\"Training set accuracy: {accuracy(rf, X_train, y_train)}\")\n",
    "print(f\"Validation set accuracy: {accuracy(rf, X_val, y_val)}\")\n",
    "print(f\"Test set accuracy: {accuracy(rf, X_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
