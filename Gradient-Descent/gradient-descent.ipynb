{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a6f8ecc-c4c2-4035-8163-2d9dbec3c521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm #optional, if you do not want to import remove tqdm() from loops!\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994d943d-9359-43ad-9930-2903c8c6c45e",
   "metadata": {},
   "source": [
    "# 1. Preprocessing\n",
    "\n",
    "We again will be using the MNIST dataset. This time I prepared the dataset as a npy file. We will load the data visualize an example and the implement logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cc8baa6-58c4-498f-a6c0-e6f491d4e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change if the file is in a different directory\n",
    "f_features = \"features.npy\"\n",
    "\n",
    "# change if the file is in a different directory\n",
    "f_labels = \"labels.npy\"\n",
    "\n",
    "# load the data\n",
    "features=np.load(f_features)\n",
    "labels=np.load(f_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ce6e785-2721-43a2-8246-d7920c5d7e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7bbdb5af8d10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(features[0,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec6857c9-458c-4fe0-92c9-1b69a2491646",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_index = 4800\n",
    "offset = 600\n",
    "validation_index = batch_index + offset\n",
    "test_index = validation_index + offset\n",
    "# As you know we need to split the data into training, validation and test\n",
    "x_train=features[0:batch_index,:,:]\n",
    "x_train=x_train.reshape((batch_index, 784))\n",
    "y_train=labels[0:batch_index].astype(int)\n",
    "\n",
    "x_val=features[batch_index:validation_index,:,:]\n",
    "x_val=x_val.reshape((offset, 784))\n",
    "y_val=labels[batch_index:validation_index].astype(int)\n",
    "\n",
    "x_test=features[validation_index:test_index,:,:]\n",
    "x_test=x_test.reshape((offset, 784))\n",
    "y_test=labels[validation_index:test_index].astype(int)\n",
    "\n",
    "# Note: Normally the split has to be random and stratified for the validation set and random for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4558976-9c71-4ef2-b8b3-710df7773806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7bbdb55e4170>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(x_train[0].reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e1667f-71d4-4563-920b-697305c06659",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression\n",
    "\n",
    "From the lecture we know that logistic regression is given by affined transformation of the data followed by applying the sigmoid function. Our first step is to implement the function we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc771958-1230-4e6f-aaa0-934be5c74c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(x, w):\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : feature tensor of dimension (N,M)\n",
    "    w : learnable parameters of dimension (M+1, C)\n",
    "\n",
    "    N is the number of samples, M the number of features and C the number of classes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : output tensor of dimension (N, C)\n",
    "\n",
    "    res should be the result of the matrix multiplication of an expanded feature tensor (1 column) with\n",
    "    the learnable parameters.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    x_bias = np.column_stack([x, np.ones((x.shape[0], 1))])\n",
    "    res = x_bias @ w\n",
    "\n",
    "    return x_bias, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3442ce9b-1a33-4485-8593-9db7763e8ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.]]),\n",
       " array([[ 78.36470687,  78.36470687,  78.36470687, ...,  78.36470687,\n",
       "          78.36470687,  78.36470687],\n",
       "        [114.62745199, 114.62745199, 114.62745199, ..., 114.62745199,\n",
       "         114.62745199, 114.62745199],\n",
       "        [140.68627563, 140.68627563, 140.68627563, ..., 140.68627563,\n",
       "         140.68627563, 140.68627563],\n",
       "        ...,\n",
       "        [167.23921684, 167.23921684, 167.23921684, ..., 167.23921684,\n",
       "         167.23921684, 167.23921684],\n",
       "        [105.94117744, 105.94117744, 105.94117744, ..., 105.94117744,\n",
       "         105.94117744, 105.94117744],\n",
       "        [ 78.42353026,  78.42353026,  78.42353026, ...,  78.42353026,\n",
       "          78.42353026,  78.42353026]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "w=np.ones((28*28+1,10)) # 28*28 are the number of features and the bias leads to +1\n",
    "res=layer(x_train, w)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8333ed-eb24-45f0-b7c0-312a0c8b30b8",
   "metadata": {},
   "source": [
    "## 2.1 Loss Function\n",
    "In exercise sheet 0, we just guessed values, but now we are smarter! First we need to define an appropriate loss function. The dataset has ten target classes, so we want to implement cross-entropy loss:\n",
    "\n",
    "$\\mathcal{L}=\\sum_{y}1\\{\\hat{y}=y\\}(-\\log[p(y)])$ \n",
    "\n",
    "The $p(y)$ is given by the softmax function\n",
    "\n",
    "$p(y_i)=\\frac{e^{x_i}}{\\sum_ie^{x_i}}$\n",
    "\n",
    "So the softmax should return a vector representing the probability of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11f9389dc31e0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(x,w,y):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    -----------\n",
    "    x : feature tensor of dimension (N,M+1)\n",
    "    w : learnable parameters of dimension (M+1, C)\n",
    "    y : target tensor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss function specified above\n",
    "    \"\"\"\n",
    "    scores = x @ w\n",
    "    probs = softmax(scores)\n",
    "    log_likelihood = -np.log(probs[range(len(y)), y])\n",
    "    return np.sum(log_likelihood) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26102408-2311-48ef-87e7-97b5537c3c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : Prediction tensor of dimension (N, C). \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : Softmax transformed tensor of dimension (N,C)\n",
    "\n",
    "    res should be the result of the softmax transformation of y.\n",
    "\n",
    "    \"\"\"\n",
    "    exp_y = np.exp(y-np.max(y, axis=1, keepdims=True))\n",
    "    return exp_y/exp_y.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95bcbdb3-674a-4a11-8e49-4c4dbbf98d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, w):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : feature tensor of dimension (N,M)\n",
    "    w : learnable parameters of dimension (M+1, C)\n",
    "\n",
    "    N is the number of samples, M the number of features and C the number of classes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : Prediction tensor of dimension (N,1)\n",
    "\n",
    "    res should be the classification of our model \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    x_bias, logits = layer(x, w)\n",
    "\n",
    "    prob = softmax(logits)\n",
    "\n",
    "    res = np.argmax(prob, axis=1).reshape(-1, 1)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888da5b-2695-43dd-be4f-c654942b1703",
   "metadata": {},
   "source": [
    "## 2.2 Optimization\n",
    "We have already learned about optimization algorithms. In this exercise we want to learn more about gradient descent, stochastic gradient descent and Newtonâ€™s method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee3da02-c0ab-4744-9c6d-105752822069",
   "metadata": {},
   "source": [
    "### 2.2.1 Gradient Descent\n",
    "For gradient descent we need to updates our parameters using the steepest descent of the gradient with respect to the parameter. It is given by the equation:\n",
    "\n",
    "$w_{n+1}=w_n-\\epsilon_n\\nabla\\mathcal{L(w_n)}$\n",
    "\n",
    "$\\epsilon_n$ is the learning rate and a hyperparameter of our optimization approach. We can calculate the gradient by using the composition rule for derivatives.\n",
    "\n",
    "The challenge is to broadcast the right dimensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "677f68ab-a9f4-4c97-8d96-49ff6dbd9a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classTensor(y, C):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : class vector of dimension N containing the true classes\n",
    "    C : number of classes\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : class tensor of dimension (N,C)\n",
    "\n",
    "    We want to transform the vector into a binary tensor. If res_ij=1 then it means that at sample i we have class j. Otherwise \n",
    "    res_ij=0.\n",
    "\n",
    "    \"\"\"  \n",
    "\n",
    "    N = len(y)\n",
    "    res = np.zeros((N, C))\n",
    "\n",
    "    for i in range(N):\n",
    "        res[i, y[i]] = 1\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf0bf99f-0a75-4f6b-94af-86cdfb26cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, w, lr):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : feature tensor of dimension (N,M)\n",
    "    y : class vector of dimension N containing the true classes\n",
    "    w : learnable parameters of dimension (M+1, C)\n",
    "    lr: learning rate of the gradient descent\n",
    "\n",
    "    N is the number of samples, M the number of features and C the number of classes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    w : updated learnable parameters\n",
    "\n",
    "    \"\"\"\n",
    "    x_bias, logits = layer(x, w)\n",
    "\n",
    "    prob_logits = softmax(logits)\n",
    "\n",
    "    y_one_hot = classTensor(y, w.shape[1])\n",
    "\n",
    "    gradient = x_bias.T @ (prob_logits - y_one_hot)\n",
    "    w  = w - lr * gradient\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd4ca5c-6bc3-4aa4-ba35-d1b34d6a887a",
   "metadata": {},
   "source": [
    "### 2.2.2 Stochastic Gradient Descent\n",
    "In this section implement stochastic gradient descent by writing the function \"def stochasticGradientDescent(...)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7f5d38b-9254-4260-98e8-6a93c96202f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T13:03:00.610083Z",
     "start_time": "2024-11-07T13:03:00.587772Z"
    }
   },
   "outputs": [],
   "source": [
    "def stochasticGradientDescent(x, y, w, lr, batch_size):\n",
    "   \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : feature tensor of dimension (N,M)\n",
    "    y : class vector of dimension N containing the true classes\n",
    "    w : learnable parameters of dimension (M+1, C)\n",
    "    lr: learning rate of the stochastic gradient descent\n",
    "    batch_size : batch size of the stochastic gradient descent\n",
    "\n",
    "    N is the number of samples, M the number of features and C the number of classes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    w : updated learnable parameters\n",
    "\n",
    "    \"\"\"\n",
    "   N = x.shape[0]\n",
    "   indices = np.random.permutation(N)\n",
    "\n",
    "   for i in range(0, N-batch_size, batch_size):\n",
    "       batch_indices = indices[i:i + batch_size]\n",
    "       x_batch = x[batch_indices]\n",
    "       y_batch = y[batch_indices]\n",
    "\n",
    "       x_bias, logits = layer(x_batch, w)\n",
    "\n",
    "       prob_logits = softmax(logits)\n",
    "\n",
    "       y_one_hot = classTensor(y_batch, w.shape[1])\n",
    "\n",
    "       gradient = x_bias.T @ (prob_logits - y_one_hot)\n",
    "\n",
    "       w = w - lr * gradient\n",
    "\n",
    "   return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab1fa70-d69a-4c19-b7f2-46c8109e8750",
   "metadata": {},
   "source": [
    "### 2.2.3 Newtonâ€™s method\n",
    "In this section implement Newton's method by writing the function \"def newtonMethod(...)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1553e22f-b7ca-4ca5-aee6-8a4e29ef6385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T13:03:03.399482Z",
     "start_time": "2024-11-07T13:03:03.372628Z"
    }
   },
   "outputs": [],
   "source": [
    "def newtonMethod(x, y, w, max_iter=100, tol=1e-3, reg_lambda=1e-5, show=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : feature tensor of dimension (N,M)\n",
    "    y : class vector of dimension N containing the true classes\n",
    "    w : learnable parameters of dimension (M+1, C)\n",
    "    max_iter : maximum number of iterations\n",
    "    tol : tolerance for convergence\n",
    "\n",
    "    N is the number of samples, M the number of features and C the number of classes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    w : updated learnable parameters\n",
    "\n",
    "    \"\"\"\n",
    "    validation_acc = []\n",
    "    train_acc = []\n",
    "\n",
    "    N, M = x.shape\n",
    "    M += 1 # bias term\n",
    "    C = w.shape[1]\n",
    "    losses = []\n",
    "    for iteration  in tqdm(range(max_iter)):\n",
    "        x_bias, logits = layer(x, w)\n",
    "\n",
    "        prob_logits = softmax(logits)\n",
    "\n",
    "        y_one_hot = classTensor(y, C)\n",
    "\n",
    "        g = x_bias.T @ (prob_logits - y_one_hot)\n",
    "\n",
    "        loss = compute_loss(x_bias, w, y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        H = x_bias.T @ x_bias + reg_lambda * np.eye(M) # we do this for numerical stability\n",
    "        H_inv = np.linalg.inv(H)\n",
    "\n",
    "        w_update = H_inv @ g\n",
    "        w = w - w_update\n",
    "\n",
    "\n",
    "        # This here is not part of the training method and it's only for the visualisation and validation\n",
    "        # I know it's implemented poorly and it could cause issues easily, but it was the easiest way to implement the visualisation\n",
    "        if show:\n",
    "            train_acc.append(accuracy_score(y_train, model(x_train, w)))\n",
    "            validation_acc.append(accuracy_score(y_val, model(x_val,w)))\n",
    "        if loss <= tol or np.linalg.norm(g) < tol:\n",
    "            print(f\"Converged at iteration {iteration}\")\n",
    "            break\n",
    "\n",
    "    if show:\n",
    "        return w, losses, train_acc, validation_acc\n",
    "    return w, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24684d6f2275d992",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T13:03:11.141291Z",
     "start_time": "2024-11-07T13:03:04.059789Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:05<00:00, 18.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "limit = np.sqrt(6 / (785 + 10))\n",
    "w = np.random.uniform(-0.01, 0.01, (785, 10))\n",
    "w, losses = newtonMethod(x_train, y_train, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b1f24d50e88ec52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T13:03:11.268325Z",
     "start_time": "2024-11-07T13:03:11.167344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7bbdb55e4b30>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax.plot(losses, c=\"orange\", label=\"val\", lw=1.5)\n",
    "ax.grid(color='gray', linestyle='dashed', alpha=0.3)\n",
    "ax.legend(loc=\"lower right\", fontsize=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7fc664-a615-441d-bf07-235a6fb23a2e",
   "metadata": {},
   "source": [
    "# 3. Training\n",
    "Now use the MNIST dataset to train a classifier and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572200c7-7fa2-4312-953a-ae98ff6d5121",
   "metadata": {},
   "source": [
    "# 3.1 Train and Plot Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f973b7dd-41a7-4486-b914-7a41360ee73c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T13:03:13.818926Z",
     "start_time": "2024-11-07T13:03:11.292166Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 54.57it/s]\n"
     ]
    }
   ],
   "source": [
    "acc_train=[]\n",
    "acc_val=[]\n",
    "\n",
    "n_epochs=100\n",
    "learningRate=0.001\n",
    "\n",
    "w = np.random.uniform(-0.01, 0.01, (785, 10))\n",
    "\n",
    "for e in tqdm(range(n_epochs)):\n",
    "\n",
    "    w = gradientDescent(x_train, y_train, w, learningRate)\n",
    "    pred = model(x_train, w)\n",
    "\n",
    "    acc_train.append(accuracy_score(y_train, pred))\n",
    "    acc_val.append(accuracy_score(y_val, model(x_val,w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "507856a8-a582-462f-86a1-33f2b787cff6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T13:03:13.910223Z",
     "start_time": "2024-11-07T13:03:13.831363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7bbdb55f5160>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax.plot(acc_val, c=\"orange\", label=\"val\", lw=1.5)\n",
    "ax.plot(acc_train, c=\"blue\", label=\"train\",lw=1.5)\n",
    "ax.grid(color='gray', linestyle='dashed', alpha=0.3)\n",
    "ax.legend(loc=\"lower right\", fontsize=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c448ab-9115-4691-85ee-e100287cafc0",
   "metadata": {},
   "source": [
    "## 3.2 Implement training for stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90ebada2-052f-4023-ad27-8634a11e69b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T13:03:17.144155Z",
     "start_time": "2024-11-07T13:03:13.926030Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 45.00it/s]\n"
     ]
    }
   ],
   "source": [
    "sgd_train_acc = []\n",
    "sgd_val_acc = []\n",
    "\n",
    "lr = 0.001\n",
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "w = np.random.uniform(-0.01, 0.01, (785, 10))\n",
    "\n",
    "\n",
    "for e in tqdm(range(n_epochs)):\n",
    "\n",
    "    w = stochasticGradientDescent(x_train, y_train, w, lr, batch_size)\n",
    "    pred = model(x_train, w)\n",
    "\n",
    "    sgd_train_acc.append(accuracy_score(y_train, pred))\n",
    "    sgd_val_acc.append(accuracy_score(y_val, model(x_val,w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0efef086-15fb-4f12-9f5b-f6191e9452ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T13:03:17.249066Z",
     "start_time": "2024-11-07T13:03:17.163319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7bbdb250dd00>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax.plot(sgd_val_acc, c=\"orange\", label=\"val\", lw=1.5)\n",
    "ax.plot(sgd_train_acc, c=\"blue\", label=\"train\",lw=1.5)\n",
    "ax.grid(color='gray', linestyle='dashed', alpha=0.3)\n",
    "ax.legend(loc=\"lower right\", fontsize=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5b9f06-9c7b-436d-9183-f3a80135835c",
   "metadata": {},
   "source": [
    "## 3.3 Implement training for newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee7ded60-0d06-44ed-9789-2a451bf4fc33",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-07T13:09:55.791015Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:05<00:00, 18.07it/s]\n"
     ]
    }
   ],
   "source": [
    "w = np.random.uniform(-0.01, 0.01, (785, 10))\n",
    "w, losses, newton_train_acc, newton_val_acc = newtonMethod(x_train, y_train, w, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec27a630-6f5d-4659-abc3-e5a0a0c2901c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T13:03:25.512389Z",
     "start_time": "2024-11-07T13:03:25.420279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7bbdb25851f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax.plot(newton_val_acc, c=\"orange\", label=\"val\", lw=1.5)\n",
    "ax.plot(newton_train_acc, c=\"blue\", label=\"train\",lw=1.5)\n",
    "ax.grid(color='gray', linestyle='dashed', alpha=0.3)\n",
    "ax.legend(loc=\"lower right\", fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4677349bf888c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
